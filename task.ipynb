{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf7-AXNc3aEs"
   },
   "source": [
    "# Computer Vision\n",
    "\n",
    "In the following you will import a public dataset and write python classes to use this dataset for training a neural network. Afterwards, you will deploy a neural network, train it, and also evaluate its performance. Training of neural networks works much faster on GPUs. These are available on Colab. To make sure you can access it:\n",
    "\n",
    "1. In the above menu, select `Runtime` > `Change runtime type`\n",
    "2. In `Hardware accelerator` box, select GPU\n",
    "3. Again, in above menu, select `Runtime` > `Restart runtime`...\n",
    "4. Rerun `!nvidia-smi` line below and confirm GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGbTHYnyJfpS"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu2PtUXLIDRH"
   },
   "source": [
    "## Exercise 1 - The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "975gFzK7_72c"
   },
   "source": [
    "### 1. Let's copy the dataset.\n",
    "\n",
    "We will use a publicly available dataset called [PanNuke](https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke). Uploading the dataset to colab takes time and each time your runtime in colab is disrupted these files will be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6Ft0aLVJ99r"
   },
   "source": [
    "#### Upload the file to your google drive and mount it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F37-dKrg6UbO"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM2CL02yBX7G"
   },
   "source": [
    "Afterwards, please create a folder `folds` in your root folder on google drive and upload `fold1.zip` inside the folder.\n",
    "Let's see what is in that folder and unzip the large zip file into the colab storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxJst2BY3fPb"
   },
   "outputs": [],
   "source": [
    "!ls -l /content/gdrive/MyDrive/folds\n",
    "\n",
    "!unzip /content/gdrive/MyDrive/folds/fold1.zip -d /content/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue-opmR4CD_u"
   },
   "source": [
    "### 2. Let's look at the data\n",
    "The zip contains 3 binary files: images, masks, and types. Each of the files contains one big array encoded using `numpy` python library. Let's see what each file contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PQoiVzrK_NX7"
   },
   "outputs": [],
   "source": [
    "# First a few imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os, random\n",
    "#\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1LgN8Rv_n9g"
   },
   "outputs": [],
   "source": [
    "imgs = np.load(\"/content/fold1/images.npy\")\n",
    "print(imgs.shape)\n",
    "print(imgs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5yTK_HUTHkqd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2656,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Breast', 'Breast', 'Breast', 'Breast', 'Breast', 'Breast',\n",
       "       'Breast', 'Breast', 'Breast', 'Breast'], dtype='<U13')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps = np.load(r\"C:\\Users\\Danish Hussain\\Downloads\\fold_1\\Fold 1\\images\\fold1/types.npy\")\n",
    "print(tps.shape)\n",
    "tps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmoIhcgpE0Pg"
   },
   "outputs": [],
   "source": [
    "masks = np.load(\"/content/fold1/masks.npy\")\n",
    "print(masks.shape)\n",
    "print(masks.dtype)\n",
    "print(np.max(masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zj5lty17D32h"
   },
   "source": [
    "We need to reshape the data arrays into `[N,C,H,W]` format that is needed for the ML models we will develop during this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k8Uw_daD8hj"
   },
   "outputs": [],
   "source": [
    "imgs = np.rollaxis(imgs, 3, 1)\n",
    "masks = np.rollaxis(masks, 3, 1)\n",
    "print(imgs.shape)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U1X4gDBC47t"
   },
   "source": [
    "The following arrays represent images, types of tissue, and semgnetation masks. Segmentation masks mark areas of each cell with a different (integer) number. There are 6 cell types in this dataset which we need to define in a separate array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1PLiITbx0wM"
   },
   "outputs": [],
   "source": [
    "CELL_TYPES = [\"neoplastic\", \"inflammatory\", \"softtissue\", \"dead\", \"epithelial\", \"any\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw-W5vPWFDUt"
   },
   "source": [
    "We will first view some example segmentation masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y_sX5AN_ZsU"
   },
   "outputs": [],
   "source": [
    "def cmap_discrete():\n",
    "    cmap = plt.cm.jet  # define the colormap\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    random.shuffle(cmaplist)\n",
    "    cmaplist = [(0, 0, 0, 1.0)] + cmaplist\n",
    "    # create the new map\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list('cmap', cmaplist, cmap.N+1)\n",
    "    return cmap\n",
    "\n",
    "def show_example_images(imgs, masks, tps, n=10):\n",
    "    plt.figure(figsize=(12, n*2))\n",
    "    indices = [x for x in range(len(masks))]\n",
    "    indices = random.sample(indices, k=n)\n",
    "    cmap = cmap_discrete()\n",
    "    for i, ind in enumerate(indices):\n",
    "        mask = masks[ind]\n",
    "        tissue = tps[ind]\n",
    "        img = imgs[ind]\n",
    "        _ = plt.subplot(n, 7, 7*i+1)\n",
    "        plt.imshow(np.rollaxis(img,0,3))\n",
    "        plt.title(tissue, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "        for j in range(len(CELL_TYPES)):\n",
    "            _ = plt.subplot(n, 7, 7*i + 2 + j)\n",
    "            plt.imshow(mask[j,:,:],  cmap=cmap, interpolation='nearest')\n",
    "            plt.title(tissue + \" \" + CELL_TYPES[j], fontsize=8)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juX34w25CutZ"
   },
   "outputs": [],
   "source": [
    "show_example_images(imgs, masks, tps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cu0sMURFtK1"
   },
   "source": [
    "Now how many images, tissue types, and cells of each type does the dataset contain? We will list and plot them in a barplot. First tissue types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKHLs8BnEHrr"
   },
   "outputs": [],
   "source": [
    "counts = np.unique(tps, return_counts=True)\n",
    "print(list(zip(counts[0],counts[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P3niAlM9VVK"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.barh(counts[0], counts[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbH66kqOGKzw"
   },
   "source": [
    "Then the same for each cell type. Last entry in the 'CELL_TYPES` is a binary segmentation map of all cell types. We will therefore omit that entry in the table in the cell counting below. We need to extract the cell count numbers from segmentation maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrVAOt2O8cXS"
   },
   "outputs": [],
   "source": [
    "cell_counts = []\n",
    "for i, ct in enumerate(CELL_TYPES[:-1]):\n",
    "  counts = np.array(list(map(lambda m: np.max(m), masks[:,i,:,:])))\n",
    "  cell_counts.append(np.sum(counts))\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.barh(CELL_TYPES[:-1], cell_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81WJTQNAG5cF"
   },
   "source": [
    "### 3. Dataset class for an ML model\n",
    "\n",
    "We will use [PyTorch](https://pytorch.org/) throughout our excercises. We will start with construction of a [dataset class](https://pytorch.org/docs/stable/data.html) that we will use for feeding the data to the deep learning model. Here is an abstract `Segmentation Dataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-A8YOkcLuc5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SegmDataset(Dataset):\n",
    "    def __init__(self, root_dir, imgs, masks, tps, augment, is_binary):\n",
    "        self.root_dir = root_dir\n",
    "        self.augment = augment\n",
    "        self.is_binary = is_binary\n",
    "\n",
    "        if root_dir != \"\":\n",
    "            self.imgs = np.load(os.path.join(root_dir, \"images.npy\"))\n",
    "            self.masks = np.load(os.path.join(root_dir, \"masks.npy\"))\n",
    "            self.tissues = np.load(os.path.join(root_dir, \"types.npy\"))\n",
    "        else:\n",
    "            self.imgs = imgs\n",
    "            self.masks = masks\n",
    "            self.tissues = tps\n",
    "        self.imgs = self.imgs.astype(np.float32) / 255\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "    def transform(self, img, mask):\n",
    "        i = random.randint(0,3)\n",
    "        if i == 1:\n",
    "            img = np.flip(img, axis=1)\n",
    "            mask = np.flip(mask, axis=1)\n",
    "        elif i == 2:\n",
    "            img = np.flip(img, axis=2)\n",
    "            mask = np.flip(mask, axis=2)\n",
    "        elif i == 3:\n",
    "            img = np.flip(np.flip(img, axis=1), axis=2)\n",
    "            mask = np.flip(np.flip(mask, axis=1), axis=2)\n",
    "\n",
    "        i = random.randint(0,3)\n",
    "        img = np.rot90(img, k=i, axes=(1,2))\n",
    "        mask = np.rot90(mask, k=i, axes=(1,2))\n",
    "\n",
    "        return img.copy(), mask.copy()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF8pl_LwKnBI"
   },
   "source": [
    "`Dataset` class implements `__getitem__` and `__len__` functions which allow to interate through it, just like we do through a list in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwnslYv_LQg5"
   },
   "source": [
    "### 4. Your task\n",
    "Implement the `__getitem__()` method. You are allowed to implement further helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmWy_FTILxhZ"
   },
   "outputs": [],
   "source": [
    "class SegmDatasetBinary(SegmDataset):\n",
    "# Class that returns a dictionary of three values:\n",
    "# {'image': image,\n",
    "#  'mask': binary segmentation mask,\n",
    "#  'tissue': name_of_tissue }\n",
    "# in the __getitem__ function\n",
    "    def __init__(self, root_dir, imgs, masks, tps, augment=True):\n",
    "        super().__init__(root_dir, imgs, masks, tps, augment, True)\n",
    "\n",
    "    ### TODO ###\n",
    "    def __getitem__(self, idx):\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4WGvemoLcFq"
   },
   "outputs": [],
   "source": [
    "class SegmDatasetMultiClass(SegmDataset):\n",
    "# Class that returns a dictionary of three values:\n",
    "# {'image': image,\n",
    "#  'mask': segmentation mask in which different cell types are marked with following integer values (neoplastic:1, inflammatory:2, softtissue:3, dead:4, epithelial:5),\n",
    "#  'tissue': name of tissue }\n",
    "# in the __getitem__ function\n",
    "    def __init__(self, root_dir, imgs, masks, tps, augment=True):\n",
    "        super().__init__(root_dir, imgs, masks, tps, augment, False)\n",
    "\n",
    "    ### TODO ###\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY69N40UN9dZ"
   },
   "source": [
    "### 5. Task 2\n",
    "\n",
    "As you saw in the barplots above, our dataset is strongly biased. Among 19 different tissues over 30% of images come from the breast. When we train a model based on such data we risk to overfit it to the breast tissue. We will now create a new abstract dataset class that will be designed such that images of all non-breast tissues are fed into the model more often than their actual image count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlBV_w5WWIk4"
   },
   "outputs": [],
   "source": [
    "class SegmDatasetBalanced(SegmDataset):\n",
    "    def __init__(self, root_dir, imgs, masks, tps, augment, is_binary, balance_to):\n",
    "        super().__init__(root_dir, imgs, masks, tps, augment, is_binary)\n",
    "        tissue_names, counts = np.unique(self.tissues, return_counts=True)\n",
    "\n",
    "        counts_max = np.max(counts)\n",
    "        ts_max = tissue_names[counts == counts_max]\n",
    "        indexes = {}\n",
    "        for ts in tissue_names:\n",
    "            ts_ind = np.where(self.tissues == ts)[0]\n",
    "            missing_n = int(counts_max * balance_to - ts_ind.size)\n",
    "            if missing_n > 0:\n",
    "              additional_ts_ind = np.random.choice(ts_ind, size=missing_n, replace=True)\n",
    "              ts_ind = np.concatenate((ts_ind, additional_ts_ind))\n",
    "            indexes[ts] = ts_ind\n",
    "        self.all_indexes = np.concatenate(list(indexes.values()))\n",
    "        np.random.shuffle(self.all_indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_indexes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct2UVRChPe_T"
   },
   "source": [
    "The meaning of parameter `balance_to` is *proportion of the breast images to which we should upsample  given tissue images*. So we can choose to upsample kidney, bladder, colon, etc such that they represent at least e.g. 50% of the number of breast tissue images.\n",
    "\n",
    "Implement the `__getitem__()` method. You are allowed to implement further helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCjmloxDPQqP"
   },
   "outputs": [],
   "source": [
    "class SegmDatasetBinaryBalanced(SegmDatasetBalanced):\n",
    "# Class that returns a dictionary of three values:\n",
    "# {'image': image,\n",
    "#  'mask': binary segmentation mask,\n",
    "#  'tissue': name_of_tissue }\n",
    "# in the __getitem__ function\n",
    "    def __init__(self, root_dir, imgs, masks, tps, augment, balance_to):\n",
    "        super().__init__(root_dir, imgs, masks, tps, augment, True, balance_to)\n",
    "\n",
    "    ### TODO ###\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "class SegmDatasetMultiClassBalanced(SegmDatasetBalanced):\n",
    "# Class that returns a dictionary of three values:\n",
    "# {'image': image,\n",
    "#  'mask': segmentation mask in which different cell types are marked with following integer values (neoplastic:1, inflammatory:2, softtissue:3, dead:4, epithelial:5),\n",
    "#  'tissue': name of tissue }\n",
    "# in the __getitem__ function\n",
    "    def __init__(self, root_dir, imgs, masks, tps, augment, balance_to):\n",
    "        super().__init__(root_dir, imgs, masks, tps, augment, False, balance_to)\n",
    "\n",
    "    ### TODO ###\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZQtEfEGQx_F"
   },
   "source": [
    "### 6. Here is how we will test your code\n",
    "\n",
    "We will use some plotting functions to inspect the data returned by the different dataset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c5aMOkiYeUW"
   },
   "outputs": [],
   "source": [
    "CELL_TYPE_COLORS = [(255,64,52), (50, 168, 82), (132, 46, 158), (46, 214, 217), (247, 235, 5)]\n",
    "\n",
    "\n",
    "def composite(background, foreground, alpha):\n",
    "    for color in range(3):\n",
    "        background[:, :, color] = alpha * foreground[:, :, color] + background[:, :, color] * (1 - alpha)\n",
    "\n",
    "    return background.astype(np.uint8)\n",
    "\n",
    "def overlay_binary_mask(img, mask):\n",
    "    mask_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.float32)\n",
    "    mask_rgb[:,:,:0] = mask*255\n",
    "    alpha = mask[:,:,0] * 0.25\n",
    "    return composite(img*255, mask_rgb, alpha)\n",
    "\n",
    "\n",
    "def overlay_binary_contours(img, mask):\n",
    "    contours, _  = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    cv2.drawContours(contour_rgb, contours, -1, CELL_TYPE_COLORS[0], 3)\n",
    "    alpha = np.zeros((mask.shape[0], mask.shape[1]))\n",
    "    alpha[np.max(contour_rgb, axis=2) > 0] = 0.75\n",
    "    return composite(img*255, contour_rgb, alpha)\n",
    "\n",
    "def overlay_multiclass_contours(img, mask):\n",
    "    contour_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(len(CELL_TYPE_COLORS)):\n",
    "        m = mask[:,:,0] == i + 1\n",
    "        contours, _  = cv2.findContours(m.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(contour_rgb, contours, -1, CELL_TYPE_COLORS[i], 3)\n",
    "    alpha = np.zeros((mask.shape[0], mask.shape[1]))\n",
    "    alpha[np.max(contour_rgb, axis=2) > 0] = 0.9\n",
    "    return composite(img*255, contour_rgb, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZD0MPPdVbI6"
   },
   "outputs": [],
   "source": [
    "def test_dset(dataset):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    n = 12\n",
    "    indices = random.sample(range(len(dataset)), k=n)\n",
    "\n",
    "    for i, ind in enumerate(indices):\n",
    "      datapoint = dataset[ind]\n",
    "      img, mask, ts = datapoint['image'], datapoint['mask'], datapoint['tissue']\n",
    "      _ = plt.subplot(n // 4, 4, i+1)\n",
    "      img = np.rollaxis(img, 0, 3)\n",
    "      mask = np.rollaxis(mask, 0, 3)\n",
    "      im = overlay_binary_contours(img,mask) if dataset.is_binary else overlay_multiclass_contours(img,mask)\n",
    "      plt.imshow(im)\n",
    "      plt.title(ts, fontsize=8)\n",
    "      plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYD5HtlNZBA6"
   },
   "outputs": [],
   "source": [
    "def plot_dset_counts(dataset):\n",
    "    tissues, counts = np.unique([dataset.tissues[i] for i in dataset.all_indexes], return_counts=True)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.barh(tissues, counts)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCGIymtjXXfm"
   },
   "outputs": [],
   "source": [
    "dataset = SegmDatasetMultiClassBalanced(\"\", imgs, masks, tps, True, 0.25)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hl9TKWL9SwEJ"
   },
   "outputs": [],
   "source": [
    "test_dset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayQHiqhgd71o"
   },
   "outputs": [],
   "source": [
    "plot_dset_counts(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN-Cykv1eKRG"
   },
   "source": [
    "### 7. Final task\n",
    "\n",
    "Now the dataset class will become more complicated: instead of images and their segmentation masks the dataset should return crops of individual cells with their cell type label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Y5rIeGj3VrP"
   },
   "outputs": [],
   "source": [
    "class SingleCellDataset(Dataset):\n",
    "# This class should return via __getitem__ function following dictionary:\n",
    "# {'image': a w x h-sized cropped image of individual cell\n",
    "# 'cell_type': type of the respective cell\n",
    "# 'tissue': tissue of origin }\n",
    "    def __init__(self, root_dir, imgs, masks, tps, augment=True, random_pad=5, margin=10, w=64, h=64):\n",
    "        \"\"\"\n",
    "        margin: margin around the cell contours\n",
    "        random_pad: maximum number by which we vary the margin size\n",
    "        w,h: width and height of the output image\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.augment = augment\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        if root_dir != \"\":\n",
    "            self.imgs = np.load(os.path.join(root_dir, \"images.npy\"))\n",
    "            self.masks = np.load(os.path.join(root_dir, \"masks.npy\"))\n",
    "            self.tissues = np.load(os.path.join(root_dir, \"types.npy\"))\n",
    "        else:\n",
    "            self.imgs = imgs\n",
    "            self.masks = masks\n",
    "            self.tissues = tps\n",
    "        self.imgs = self.imgs.astype(np.float32) / 255\n",
    "        self.tissue_types = np.unique(self.tissues)\n",
    "        self.random_pad = random_pad\n",
    "\n",
    "        ## index: (cell type, tissue) : [ (img_i, x_min, x_max, y_min, y_max), ...]\n",
    "        indices = {}\n",
    "        for ct in CELL_TYPES[:5]:\n",
    "            for tt in self.tissue_types:\n",
    "                indices[(ct, tt)] = []\n",
    "\n",
    "        for i in range(self.masks.shape[0]):\n",
    "            tt = self.tissues[i]\n",
    "            for i_ct, ct in enumerate(CELL_TYPES[:5]):\n",
    "                mask = self.masks[i,i_ct,:,:]\n",
    "                cells = np.unique(mask)\n",
    "                cells = np.sort(cells)\n",
    "                for ic in cells[1:]:\n",
    "                    ys, xs = np.where(mask == ic)\n",
    "                    x_min, x_max, y_min, y_max = np.min(xs), np.max(xs), np.min(ys), np.max(ys)\n",
    "                    if (x_min > 0) and (x_max < mask.shape[1]-1) and (y_min > 0) and (y_max < mask.shape[0] -1):\n",
    "                        x_min = max(0, x_min - margin)\n",
    "                        x_max = min(mask.shape[1], x_max + margin)\n",
    "                        y_min = max(0, y_min - margin)\n",
    "                        y_max = min(mask.shape[0], y_max + margin)\n",
    "                        indices[(ct, tt)].append((i, x_min, x_max, y_min, y_max))\n",
    "        self.indices_d = indices\n",
    "        self.indices = list(indices.values())\n",
    "        self.labels = list(indices.keys())\n",
    "        a = list(map(len, self.indices))\n",
    "        self.cs = np.cumsum(a)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cs[-1]\n",
    "\n",
    "    ### TODO ###\n",
    "    def transform(self, img):\n",
    "        pass\n",
    "\n",
    "    def pad_img(self, img):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGXJ0zgpiyZc"
   },
   "source": [
    "And this is how we will test this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rbm4PpYPZy4"
   },
   "outputs": [],
   "source": [
    "def test_singlecell_dset(dataset):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    n = 8\n",
    "    indices = random.sample(range(len(dataset)), k=n)\n",
    "\n",
    "    for i, ind in enumerate(indices):\n",
    "      datapoint = dataset[ind]\n",
    "      img, ct, ts = datapoint['image'], datapoint['cell_type'], datapoint['tissue']\n",
    "      img = (img*255)\n",
    "      img = img.astype(np.uint8)\n",
    "      img = np.rollaxis(img, 0, 3)\n",
    "      _ = plt.subplot(n*2 // 4, 4, 2*i+1)\n",
    "\n",
    "      plt.imshow(img)\n",
    "      plt.title(ct + \" \" + ts, fontsize=8)\n",
    "      plt.axis(\"off\")\n",
    "\n",
    "      datapoint = dataset[ind]\n",
    "      img, ct, ts = datapoint['image'], datapoint['cell_type'], datapoint['tissue']\n",
    "      img = (img*255)\n",
    "      img = img.astype(np.uint8)\n",
    "      img = np.rollaxis(img, 0, 3)\n",
    "      _ = plt.subplot(n*2 // 4, 4, 2*i+2)\n",
    "      plt.imshow(img)\n",
    "      plt.title(ct + \" \" + ts, fontsize=8)\n",
    "      plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgdnujfJDD6l"
   },
   "outputs": [],
   "source": [
    "sc_dataset = SingleCellDataset(\"\", imgs, masks, tps, margin=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJEzGTq0Pu7B"
   },
   "outputs": [],
   "source": [
    "test_singlecell_dset(sc_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSL6vCR-IdbB"
   },
   "source": [
    "## Exercise 2 - Model Definition and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvGq6l6HOyJK"
   },
   "source": [
    "In order to free some RAM, please execute the following cell before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7UYmTptOxfF"
   },
   "outputs": [],
   "source": [
    "del(dataset)\n",
    "del(test_dset)\n",
    "del(sc_dataset)\n",
    "del(test_singlecell_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwYL3naZOQOd"
   },
   "source": [
    "Some further imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyaOHkuIOPhr"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from random import sample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4dghJCYoDZa"
   },
   "source": [
    "### 1. Load and prepare the data\n",
    "\n",
    "We will load the data and make a train-validation-test dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsYM3SWLJ6FM"
   },
   "outputs": [],
   "source": [
    "imgs = np.load(\"/content/fold1/images.npy\")\n",
    "tps = np.load(\"/content/fold1/types.npy\")\n",
    "masks = np.load(\"/content/fold1/masks.npy\")\n",
    "imgs = np.rollaxis(imgs, 3, 1)\n",
    "masks = np.rollaxis(masks, 3, 1)\n",
    "print(imgs.shape)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYbMbohzxvVU"
   },
   "outputs": [],
   "source": [
    "rd = np.random.sample(imgs.shape[0])\n",
    "in_test = rd >=0.8\n",
    "in_val = np.logical_and(rd >=0.6, rd < 0.8)\n",
    "in_train = rd < 0.6\n",
    "imgs_train, masks_train, tissues_train =  imgs[in_train].copy(), masks[in_train].copy(), tps[in_train].copy()\n",
    "imgs_val, masks_val, tissues_val = imgs[in_val].copy(), masks[in_val].copy(), tps[in_val].copy()\n",
    "imgs_test, masks_test, tissues_test = imgs[in_test].copy(), masks[in_test].copy(), tps[in_test].copy()\n",
    "\n",
    "print(\"Train set size:      (%i, %i, %i, %i)\" % imgs_train.shape)\n",
    "print(\"Validation set size: (%i, %i, %i, %i)\" % imgs_val.shape)\n",
    "print(\"Test set size:       (%i, %i, %i, %i)\" % imgs_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dyRf80vpGKU"
   },
   "source": [
    "### 2. Define and create the network\n",
    "\n",
    "We will implement [UNet](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) in it's basic form and train it to perform semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbFBm9EyEz0Z"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=True,\n",
    "        batch_norm=True,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "\n",
    "        Args:\n",
    "            in_channels (int) : number of input channels\n",
    "            n_classes (int)   : number of output channels\n",
    "            depth (int)       : depth of the network\n",
    "            wf (int)          : number of filters in the first layer is 2**wf\n",
    "            padding (bool)    : if True, apply padding such that the input shape\n",
    "                                is the same as the output.\n",
    "                                This may introduce artifacts\n",
    "            batch_norm (bool) : Use BatchNorm after layers with an\n",
    "                                activation function\n",
    "            up_mode (str)     : one of 'upconv' or 'upsample'.\n",
    "                                'upconv' will use transposed convolutions for\n",
    "                                learned upsampling.\n",
    "                                'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "\n",
    "        # this is where the network itself is created! according to your value of\n",
    "        # 'depth', you add down_layers and then up_layers on the way back\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[:, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xySXoS4ip-vW"
   },
   "source": [
    "Here we will parametrize the network. Depending whether you perform binary of multiclass segmentation you need to set the `n_classes` parameter to 2 or 6. We also parametrize the size of UNet. For initial test runs make the network smaller to speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2tj8L7zz3Jh"
   },
   "outputs": [],
   "source": [
    "network_depth = 3 # how deep should the UNet be\n",
    "wf = 4            # number of filters in the first layer\n",
    "batch_size = 4\n",
    "num_epochs = 50   # number of training epochs\n",
    "gpuid = 0\n",
    "n_classes   = 6   # number of classes in the data mask that we'll aim to predict\n",
    "in_channels = 3   # input channel of the data, RGB = 3\n",
    "\n",
    "torch.manual_seed(10100)\n",
    "\n",
    "param_stamp = f'd{network_depth}_wf{wf}_b{batch_size}_e{num_epochs}'\n",
    "\n",
    "print(\"torch cuda is available:\", torch.cuda.is_available())\n",
    "\n",
    "# specify if we should use a GPU (cuda) or only the CPU\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_properties(gpuid))\n",
    "    torch.cuda.set_device(gpuid)\n",
    "    device = torch.device(f'cuda:{gpuid}')\n",
    "else:\n",
    "    device = torch.device(f'cpu')\n",
    "\n",
    "# build the model according to the paramters specified above and copy it to the GPU.\n",
    "# then print out the number of trainable parameters\n",
    "model = UNet(\n",
    "    n_classes   = n_classes,\n",
    "    in_channels = in_channels,\n",
    "    depth       = network_depth,\n",
    "    wf          = wf\n",
    ").to(device)\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n",
    "print(f'\\nNetwork depth: {network_depth}\\nwf: {wf}\\n'\n",
    "      f'batch size: {batch_size}\\n{num_epochs} epochs\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGlzz6Xusdhm"
   },
   "source": [
    "We have the network, we will now create instances of dataset class for each of the datasets: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXMwxvzN0Ii7"
   },
   "outputs": [],
   "source": [
    "datasets_train_val = {\"train\": SegmDatasetMultiClass(\"\", imgs_train, masks_train, tissues_train),\n",
    "                      \"val\": SegmDatasetMultiClass(\"\", imgs_val, masks_val, tissues_val) }\n",
    "dataset_test = SegmDatasetMultiClass(\"\", imgs_test, masks_test, tissues_test, augment=False)\n",
    "\n",
    "dataloaders = {}\n",
    "for phase, ds in datasets_train_val.items():\n",
    "    dataloaders[phase] = DataLoader(\n",
    "        datasets_train_val[phase],\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = True,\n",
    "        num_workers = 0,\n",
    "        pin_memory  = True\n",
    "    )\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STG3fbpMs3-k"
   },
   "source": [
    "### 3. Network training\n",
    "\n",
    "We will now train the network, two remaining things to define is the optimizer and loss function. The basic optimizer is [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) but you can also experiment with [other optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMt2YdA40WaU"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# Adam is an example alternative to SGD:\n",
    "#optim = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzzaC2qftimu"
   },
   "source": [
    "We will use [cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKv_Z9g70k2B"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)\n",
    "# reduction='none' makes sure we get a 2D output instead of a 1D \"summary\" value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxK7eAEYuHXm"
   },
   "source": [
    "And here is the training loop. In each epoch we collect and report loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6WyOHZK0rSS"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(10100)\n",
    "\n",
    "loss_log = {'train': list(), 'val': list()}\n",
    "acc_log  = {'train': list(), 'val': list()}\n",
    "\n",
    "start_time  = time.time()\n",
    "model = model.float()\n",
    "best_loss = np.Infinity\n",
    "phases = list(dataloaders.keys())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # zero out epoch based performance variables\n",
    "    all_acc  = {key: 0 for key in phases}\n",
    "    all_loss = {key: torch.zeros(0).to(device) for key in phases}\n",
    "    cmatrix  = {key: np.zeros((n_classes, n_classes)) for key in phases}\n",
    "    marker = ''\n",
    "\n",
    "    for phase in phases:  # iterate through both training and validation states\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:  # when in eval mode, we don't want parameters to be updated\n",
    "            model.eval()  # Set model to evaluate mode\n",
    "\n",
    "        for batch in dataloaders[phase]:\n",
    "            input_imgs = batch['image']\n",
    "            input_masks = batch['mask']\n",
    "\n",
    "            input_imgs = input_imgs.to(device=device, dtype=torch.float32)\n",
    "            input_masks = input_masks.to(device=device, dtype=torch.long)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):  # dynamically set gradient computation,\n",
    "                # in case of validation, this isn't needed\n",
    "                # disabling is good practice and improves inference time\n",
    "\n",
    "                prediction = model(input_imgs)  # [N, Nclass, H, W]\n",
    "                loss_matrix = criterion(prediction, torch.squeeze(input_masks, 1))\n",
    "                loss = loss_matrix.mean()  # can skip if edge weight==1\n",
    "\n",
    "                if phase == \"train\":  # in case we're in train mode, need to do back propogation\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    train_loss = loss\n",
    "\n",
    "                all_loss[phase] = torch.cat((all_loss[phase], loss.detach().view(1, -1)))\n",
    "\n",
    "                if phase == \"val\":# and not torch.isnan(loss):  # if this phase is part of validation, compute confusion matrix\n",
    "                    p = prediction[:, :, :, :].detach().cpu().numpy()\n",
    "                    cpredflat = np.argmax(p, axis=1).flatten()\n",
    "                    input_masks = input_masks.cpu().numpy().flatten()\n",
    "\n",
    "                    cmatrix[phase] = cmatrix[phase] + confusion_matrix(input_masks, cpredflat, labels=range(n_classes))\n",
    "\n",
    "        all_acc[phase] = (cmatrix[phase] / (cmatrix[phase].sum() + 0.0000001)).trace()\n",
    "        all_loss[phase] = all_loss[phase].cpu().numpy().mean()\n",
    "\n",
    "        loss_log[phase].append(all_loss[phase])\n",
    "        acc_log[phase].append(all_acc[phase])\n",
    "\n",
    "    if all_loss['val'] < best_loss:\n",
    "      marker = '***'\n",
    "      best_loss = all_loss['val']\n",
    "\n",
    "    print( '[%3d/%3d] (%3d%%)  |  LOSS - train: %.4f    val: %.4f  %s' % (\n",
    "        epoch + 1,\n",
    "        num_epochs,\n",
    "        (epoch + 1) / num_epochs * 100,\n",
    "        all_loss[\"train\"],\n",
    "        all_loss[\"val\"],\n",
    "        marker\n",
    "        ),\n",
    "        end=\"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USdDUksI5bvI"
   },
   "source": [
    "No we will plot the loss values and find which epoch showed best results in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgMx1t1wqRZQ"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(loss_log[\"val\"])\n",
    "\n",
    "print(f'Best epoch: {best_epoch} with a loss of {np.min(loss_log[\"val\"])}')\n",
    "\n",
    "plt.plot(loss_log['train'])\n",
    "plt.plot(loss_log['val'])\n",
    "plt.vlines(np.argmin(loss_log['val']), 0, 1, linestyles='dotted', alpha=.5)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE__JImc7MvD"
   },
   "source": [
    "### 4. Model evaluation\n",
    "\n",
    "Finally we will run segmentation on the test set and inspect the results. First we run the inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vxkWcMwqwQ0"
   },
   "outputs": [],
   "source": [
    "res_masks = np.zeros((imgs_test.shape[0], imgs_test.shape[2], imgs_test.shape[3]), dtype=np.uint8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch_i, batch in enumerate(test_loader):\n",
    "        batch_imgs = batch['image']\n",
    "        batch_masks = batch['mask']\n",
    "\n",
    "        batch_imgs = batch_imgs.to(device=device, dtype=torch.float32)\n",
    "        batch_masks = batch_masks.to(device=device, dtype=torch.long)\n",
    "\n",
    "        prediction = model(batch_imgs)  # [N, Nclass, H, W]\n",
    "        sem_preds = np.argmax(F.softmax(prediction, dim=1).cpu().numpy(), axis=1)\n",
    "        res_masks[batch_i*batch_size:(batch_i+1)*batch_size] = sem_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2vDMc-u7-1p"
   },
   "source": [
    "Let's plot some example segmenation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SdGyKYcjaA_"
   },
   "outputs": [],
   "source": [
    "n = 12\n",
    "plt.figure(figsize=(6, 6))\n",
    "if len(res_masks.shape) < 4:\n",
    "    res_masks = np.expand_dims(res_masks, 3)\n",
    "for i, ind in enumerate(random.sample(range(imgs_test.shape[0]), n)):\n",
    "    _ = plt.subplot(4, n // 4, i+1)\n",
    "    img = np.rollaxis(imgs_test[ind], 0, 3).astype(np.float32) / 255\n",
    "    mask = res_masks[ind]\n",
    "    im = overlay_binary_contours(img,mask) if dataset_test.is_binary else overlay_multiclass_contours(img,mask)\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"\", fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRK1iWTXp-JU"
   },
   "source": [
    "Finally we will calculate some accuracy metrics on the test set:\n",
    "- [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) for all classes\n",
    "- number of not detected cells - cells that have intersection over unition < 0.5 with any of the cells in ground truth\n",
    "- number of misclassified cells - pixels of a cell that have correct class assignment represent < 0.5 of all cell pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TreMCpV9z04M"
   },
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTpL0OS-6Cs1"
   },
   "outputs": [],
   "source": [
    "def cell_detection(masks, preds, cutoff=0.5):\n",
    "    cell_acc = np.zeros((3, n_classes)) # detected, correctly classified\n",
    "    for i, ct in enumerate(CELL_TYPES[:5]):\n",
    "        for img_i in range(masks.shape[0]):\n",
    "            mask = masks[img_i,i,:,:]\n",
    "            cns = np.unique(mask)\n",
    "            cns = cns[cns != 0]\n",
    "            pred_mask = preds[img_i]\n",
    "            for cn in cns:\n",
    "                pred_pixels = pred_mask[mask == cn]\n",
    "                cell_size = np.sum(mask == cn)\n",
    "                prop_detected = np.sum(pred_pixels > 0) / cell_size\n",
    "                prop_correct_class = np.sum(pred_pixels == i+1) / cell_size\n",
    "                cell_acc[0,i] += prop_detected >= cutoff\n",
    "                cell_acc[1,i] += prop_correct_class >= cutoff\n",
    "                cell_acc[2,i] += 1\n",
    "    return cell_acc\n",
    "\n",
    "def accuracy_metrics(masks, preds):\n",
    "    cell_acc = cell_detection(masks, preds)\n",
    "    dice_classes = np.zeros((n_classes))\n",
    "    for cl in range(n_classes):\n",
    "        y_true = (masks[:,cl,:,:] > 0).astype(np.uint8)\n",
    "        y_pred = (preds == cl+1).astype(np.uint8)\n",
    "        dice_classes[cl] = dice_coef(y_true, y_pred)\n",
    "    for i, ct in enumerate(CELL_TYPES[:5]):\n",
    "        print(\"%s\\tdice: %.3f\\tdetected: %.3f\\tcorrect class: %.3f\" % (ct.ljust(12, ' '), dice_classes[i], cell_acc[0,i]/cell_acc[2,i], cell_acc[1,i]/cell_acc[2,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RqyqzCm9Wlj"
   },
   "outputs": [],
   "source": [
    "accuracy_metrics(masks_test, np.squeeze(res_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyW0NOxZ6i6D"
   },
   "source": [
    "### 5. Your Task\n",
    "\n",
    "Experiment with the training: hyperparameters, optimizers, various versions of the dataset class, length of training, such that the segmentation on the test set is best possible. All possible postprocessing of the results is allowed. Save your notebook and network with the command below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRfn30Ax67s-"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cp.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
