{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOWMY2HtpYRcdEC/CcJscxs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SISlsjgNSL-o","executionInfo":{"status":"ok","timestamp":1721763206432,"user_tz":-120,"elapsed":2299,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"20d7f261-d915-46b5-8c83-77d9df73c251"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","\n","def verify_dataset(images_dir, labels_dir):\n","    image_files = set(file for file in os.listdir(images_dir) if file.endswith('.jpg'))\n","    label_files = set(file.replace('.txt', '.jpg') for file in os.listdir(labels_dir) if file.endswith('.txt'))\n","\n","    unmatched_images = image_files - label_files\n","    unmatched_labels = {name.replace('.jpg', '.txt') for name in (label_files - image_files)}\n","\n","    print(f\"Total images in {images_dir}: {len(image_files)}\")\n","    print(f\"Total labels in {labels_dir}: {len(label_files)}\")\n","    if unmatched_images:\n","        print(f\"Unmatched images: {unmatched_images}\")\n","    if unmatched_labels:\n","        print(f\"Unmatched labels: {unmatched_labels}\")\n","\n","base_dir = '/content/drive/My Drive/archive-5'\n","train_images_dir = os.path.join(base_dir, 'train', 'images')\n","train_labels_dir = os.path.join(base_dir, 'train', 'labels')\n","valid_images_dir = os.path.join(base_dir, 'valid', 'images')\n","valid_labels_dir = os.path.join(base_dir, 'valid', 'labels')\n","test_images_dir = os.path.join(base_dir, 'test', 'images')\n","test_labels_dir = os.path.join(base_dir, 'test', 'labels')\n","\n","verify_dataset(train_images_dir, train_labels_dir)\n","verify_dataset(valid_images_dir, valid_labels_dir)\n","verify_dataset(test_images_dir, test_labels_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IrXvLm2ZSS7s","executionInfo":{"status":"ok","timestamp":1721763210665,"user_tz":-120,"elapsed":2294,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"46a493df-ef4f-4590-bde0-decb5209a6e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total images in /content/drive/My Drive/archive-5/train/images: 4200\n","Total labels in /content/drive/My Drive/archive-5/train/labels: 4200\n","Total images in /content/drive/My Drive/archive-5/valid/images: 1704\n","Total labels in /content/drive/My Drive/archive-5/valid/labels: 1704\n","Total images in /content/drive/My Drive/archive-5/test/images: 100\n","Total labels in /content/drive/My Drive/archive-5/test/labels: 100\n"]}]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def analyze_class_distribution(labels_dir):\n","    class_counts = defaultdict(int)\n","    label_files = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]\n","\n","    for file_name in label_files:\n","        file_path = os.path.join(labels_dir, file_name)\n","        with open(file_path, 'r') as file:\n","            for line in file.readlines():\n","                class_id = int(line.split()[0])\n","                class_counts[class_id] += 1\n","\n","    return class_counts\n","\n","print(\"Training Set Class Distribution:\")\n","print(analyze_class_distribution(train_labels_dir))\n","print(\"\\nValidation Set Class Distribution:\")\n","print(analyze_class_distribution(valid_labels_dir))\n","print(\"\\nTest Set Class Distribution:\")\n","print(analyze_class_distribution(test_labels_dir))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUF0geD6SfmP","executionInfo":{"status":"ok","timestamp":1721763345534,"user_tz":-120,"elapsed":133531,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"195936d6-9c8d-4e0f-b663-693033092886"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Set Class Distribution:\n","defaultdict(<class 'int'>, {12: 2228, 17: 1419, 1: 1318, 2: 821, 5: 662, 6: 2214, 4: 714, 15: 371, 9: 356, 7: 583, 16: 304, 0: 142, 10: 751, 8: 252, 3: 376, 11: 369, 14: 225, 13: 145})\n","\n","Validation Set Class Distribution:\n","defaultdict(<class 'int'>, {7: 186, 14: 125, 12: 854, 13: 90, 4: 267, 1: 459, 8: 93, 5: 263, 11: 178, 17: 569, 6: 565, 2: 320, 10: 265, 0: 62, 15: 120, 16: 113, 9: 178, 3: 123})\n","\n","Test Set Class Distribution:\n","defaultdict(<class 'int'>, {12: 59, 10: 28, 17: 38, 1: 26, 5: 30, 4: 19, 14: 9, 7: 19, 13: 2, 15: 12, 6: 55, 16: 6, 11: 10, 9: 7, 0: 6, 8: 4, 3: 51, 2: 13})\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n","\n","class YOLOv7(nn.Module):\n","    def __init__(self, in_channels=3, num_classes=17):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","        self.layers = self._create_layers()\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def _create_layers(self):\n","        layers = []\n","        in_channels = self.in_channels\n","\n","        architecture = [\n","            (7, 64, 2, 3), \"M\",\n","            (3, 192, 1, 1), \"M\",\n","            (1, 128, 1, 0), (3, 256, 1, 1), (1, 256, 1, 0), (3, 512, 1, 1), \"M\",\n","            [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n","            (1, 512, 1, 0), (3, 1024, 1, 1), \"M\",\n","            [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n","            (3, 1024, 1, 1), (3, 1024, 2, 1), (3, 1024, 1, 1), (3, 1024, 1, 1)\n","        ]\n","\n","        for x in architecture:\n","            if type(x) == tuple:\n","                layers.append(CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3]))\n","                in_channels = x[1]\n","            elif type(x) == str:\n","                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n","            elif type(x) == list:\n","                conv1, conv2, num_repeats = x\n","                for _ in range(num_repeats):\n","                    layers.append(CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3]))\n","                    layers.append(CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3]))\n","                    in_channels = conv2[1]\n","\n","        layers.append(nn.Flatten())\n","        # Calculating the output size assuming the input image size is divisible by 32\n","        output_grid_size = 7  # This is a simplification, the actual grid size depends on input dimensions\n","        layers.append(nn.Linear(1024 * output_grid_size * output_grid_size, output_grid_size * output_grid_size * (self.num_classes + 5 * 2)))  # B=2\n","\n","        return nn.Sequential(*layers)\n","\n","# Example use\n","model = YOLOv7()\n","print(model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDD9caDvTVm0","executionInfo":{"status":"ok","timestamp":1721763349002,"user_tz":-120,"elapsed":1493,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"610d7eb1-8c32-4a91-da04-37f3b64b8a85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["YOLOv7(\n","  (layers): Sequential(\n","    (0): CNNBlock(\n","      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (2): CNNBlock(\n","      (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): CNNBlock(\n","      (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (5): CNNBlock(\n","      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (6): CNNBlock(\n","      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (7): CNNBlock(\n","      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (9): CNNBlock(\n","      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (10): CNNBlock(\n","      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (11): CNNBlock(\n","      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (12): CNNBlock(\n","      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (13): CNNBlock(\n","      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (14): CNNBlock(\n","      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (15): CNNBlock(\n","      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (16): CNNBlock(\n","      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (17): CNNBlock(\n","      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (18): CNNBlock(\n","      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (20): CNNBlock(\n","      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (21): CNNBlock(\n","      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (22): CNNBlock(\n","      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (23): CNNBlock(\n","      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (24): CNNBlock(\n","      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (25): CNNBlock(\n","      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (26): CNNBlock(\n","      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (27): CNNBlock(\n","      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (leakyrelu): LeakyReLU(negative_slope=0.1)\n","    )\n","    (28): Flatten(start_dim=1, end_dim=-1)\n","    (29): Linear(in_features=50176, out_features=1323, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n","\n","class YOLOv7(nn.Module):\n","    def __init__(self, in_channels=3, num_classes=17):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            CNNBlock(in_channels, 64, kernel_size=7, stride=2, padding=3),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(64, 192, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(192, 128, kernel_size=1),\n","            CNNBlock(128, 256, kernel_size=3, padding=1),\n","            CNNBlock(256, 256, kernel_size=1),\n","            CNNBlock(256, 512, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(512, 1024, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(1024, 1024, kernel_size=3, padding=1),\n","            CNNBlock(1024, 1024, kernel_size=3, padding=1),\n","            nn.Flatten()\n","        )\n","        self.fc = self._create_fcs(1024, num_classes)\n","\n","    def _create_fcs(self, final_conv_out_channels, num_classes):\n","        dummy_input = torch.zeros(1, 3, 416, 416)\n","        with torch.no_grad():\n","            dummy_output = self.layers(dummy_input)\n","        flattened_size = dummy_output.shape[1]\n","        return nn.Sequential(\n","            nn.Linear(flattened_size, 4096),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(4096, 7*7*(num_classes + 5*2))  # for grid 7x7, 2 boxes per grid, and num_classes classes\n","        )\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return self.fc(x)\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.images = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith('.jpg')]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.images[idx]\n","        label_path = os.path.join(self.label_dir, img_path.split('/')[-1].replace('.jpg', '.txt'))\n","        image = Image.open(img_path).convert('RGB')\n","        boxes = []\n","        with open(label_path, 'r') as f:\n","            for line in f:\n","                boxes.append(list(map(float, line.strip().split())))\n","        if self.transform:\n","            image = self.transform(image)\n","        boxes = torch.tensor(boxes)\n","        return image, boxes\n","\n","def collate_fn(batch):\n","    images, targets = zip(*batch)\n","    images = torch.stack(images)\n","    targets = [torch.as_tensor(t, dtype=torch.float32) for t in targets]\n","    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-1)\n","    return images, targets\n","\n","transform = transforms.Compose([\n","    transforms.Resize((416, 416)),\n","    transforms.ToTensor(),\n","])\n","\n","base_dir = '/content/drive/My Drive/archive-5'\n","train_images_dir = os.path.join(base_dir, 'train', 'images')\n","train_labels_dir = os.path.join(base_dir, 'train', 'labels')\n","train_dataset = CustomDataset(train_images_dir, train_labels_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","\n","model = YOLOv7(num_classes=17)\n","model.train()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","def train(model, loader, optimizer, epochs=5):\n","    model.train()\n","    for epoch in range(epochs):\n","        for images, targets in loader:\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = yolo_loss(outputs, targets, model)\n","            loss.backward()\n","            optimizer.step()\n","            print(f'Epoch {epoch+1}: Loss {loss.item()}')\n","\n","def yolo_loss(outputs, targets, model):\n","    return ((outputs - targets)**2).mean()  # Simplified for now, needs proper implementation\n","\n","train(model, train_loader, optimizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"5v_SNL9lHzx5","executionInfo":{"status":"error","timestamp":1721773807747,"user_tz":-120,"elapsed":22187,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"35334879-e645-466e-84cd-8e2ce2c78230"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (1323) must match the size of tensor b (5) at non-singleton dimension 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-3bafe7739c42>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Simplified for now, needs proper implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-3bafe7739c42>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, epochs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3bafe7739c42>\u001b[0m in \u001b[0;36myolo_loss\u001b[0;34m(outputs, targets, model)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0myolo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Simplified for now, needs proper implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1323) must match the size of tensor b (5) at non-singleton dimension 2"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n","\n","class YOLOv7(nn.Module):\n","    def __init__(self, in_channels=3, num_classes=17):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            CNNBlock(in_channels, 64, kernel_size=7, stride=2, padding=3),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(64, 192, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(192, 128, kernel_size=1),\n","            CNNBlock(128, 256, kernel_size=3, padding=1),\n","            CNNBlock(256, 256, kernel_size=1),\n","            CNNBlock(256, 512, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(512, 1024, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(1024, 1024, kernel_size=3, padding=1),\n","            CNNBlock(1024, 1024, kernel_size=3, padding=1),\n","            nn.Flatten()\n","        )\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(173056, 4096),  # Adjusted based on the debug print\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(4096, 7*7*(num_classes + 5*2))  # Grid size * Grid size * (Classes + 5 * Bounding boxes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        print(f\"Shape before FC layers: {x.shape}\")  # Debug statement\n","        x = self.fc_layers(x)\n","        return x\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.images = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith('.jpg')]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.images[idx]\n","        label_path = os.path.join(self.label_dir, img_path.split('/')[-1].replace('.jpg', '.txt'))\n","        image = Image.open(img_path).convert('RGB')\n","        labels = []\n","        with open(label_path, 'r') as f:\n","            for line in f:\n","                labels.append(list(map(float, line.strip().split())))\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, labels\n","\n","def collate_fn(batch):\n","    images, labels = zip(*batch)\n","    images = torch.stack(images)\n","    max_boxes = max(len(label) for label in labels)\n","    padded_labels = torch.zeros((len(labels), max_boxes, 5))  # Assume 5 elements per label\n","    for i, label in enumerate(labels):\n","        if label:\n","            padded_labels[i, :len(label)] = torch.tensor(label)\n","    return images, padded_labels\n","\n","transform = transforms.Compose([\n","    transforms.Resize((416, 416)),\n","    transforms.ToTensor(),\n","])\n","\n","base_dir = '/content/drive/My Drive/archive-5'\n","train_images_dir = os.path.join(base_dir, 'train', 'images')\n","train_labels_dir = os.path.join(base_dir, 'train', 'labels')\n","train_dataset = CustomDataset(train_images_dir, train_labels_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","\n","model = YOLOv7(num_classes=17)\n","model.train()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","def train(model, loader, optimizer, epochs=5):\n","    for epoch in range(epochs):\n","        for images, targets in loader:\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = ((outputs - targets)**2).mean()  # Placeholder for actual YOLO loss\n","            loss.backward()\n","            optimizer.step()\n","            print(f'Epoch {epoch+1}: Loss {loss.item()}')\n","\n","train(model, train_loader, optimizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":343},"id":"0QMy9zWKuTJc","executionInfo":{"status":"error","timestamp":1721776628942,"user_tz":-120,"elapsed":23917,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"c728ce9e-fba6-42db-e3c1-7e402fb8dd44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape before FC layers: torch.Size([16, 173056])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (1323) must match the size of tensor b (5) at non-singleton dimension 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-881f4272ecb7>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}: Loss {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-881f4272ecb7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, epochs)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Placeholder for actual YOLO loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1323) must match the size of tensor b (5) at non-singleton dimension 2"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n","\n","class YOLOv7(nn.Module):\n","    def __init__(self, in_channels=3, num_classes=17):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            CNNBlock(in_channels, 64, kernel_size=7, stride=2, padding=3),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(64, 192, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(192, 128, kernel_size=1),\n","            CNNBlock(128, 256, kernel_size=3, padding=1),\n","            CNNBlock(256, 256, kernel_size=1),\n","            CNNBlock(256, 512, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(512, 1024, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            CNNBlock(1024, 1024, kernel_size=3, padding=1),\n","            CNNBlock(1024, 1024, kernel_size=3, padding=1),\n","            nn.Flatten()\n","        )\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(1024 * 6 * 6, 4096),  # Based on flattened output from previous statement\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(4096, 7*7*(num_classes + 5*2))  # For 7x7 grid, num_classes, and 5 bbox attributes\n","        )\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = self.fc_layers(x)\n","        return x.view(-1, 7, 7, num_classes + 5*2)  # Reshape to match target format\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.images = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith('.jpg')]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.images[idx]\n","        label_path = os.path.join(self.label_dir, img_path.split('/')[-1].replace('.jpg', '.txt'))\n","        image = Image.open(img_path).convert('RGB')\n","        labels = []\n","        with open(label_path, 'r') as f:\n","            for line in f:\n","                labels.append(list(map(float, line.strip().split())))\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, labels\n","\n","def collate_fn(batch):\n","    images, labels = zip(*batch)\n","    images = torch.stack(images)\n","    max_boxes = max(len(label) for label in labels)\n","    padded_labels = torch.zeros((len(labels), 7, 7, 5 + 17))  # Adjust for grid cells and class counts\n","    for i, label_list in enumerate(labels):\n","        for label in label_list:\n","            class_idx, x, y, w, h = label  # Assuming label format is [class_idx, x, y, w, h]\n","            grid_x = int(x * 7)\n","            grid_y = int(y * 7)\n","            padded_labels[i, grid_y, grid_x, :5] = torch.tensor([x, y, w, h, 1])\n","            padded_labels[i, grid_y, grid_x, 5 + int(class_idx)] = 1\n","\n","    return images, padded_labels.view(-1, 7*7*(17 + 5))  # Flatten to match the output shape\n","\n","transform = transforms.Compose([\n","    transforms.Resize((416, 416)),\n","    transforms.ToTensor(),\n","])\n","\n","base_dir = '/content/drive/My Drive/archive-5'\n","train_images_dir = os.path.join(base_dir, 'train', 'images')\n","train_labels_dir = os.path.join(base_dir, 'train', 'labels')\n","train_dataset = CustomDataset(train_images_dir, train_labels_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","\n","model = YOLOv7(num_classes=17)\n","model.train()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","def train(model, loader, optimizer, epochs=5):\n","    for epoch in range(epochs):\n","        for images, targets in loader:\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = ((outputs - targets)**2).mean()  # Placeholder for actual YOLO loss\n","            loss.backward()\n","            optimizer.step()\n","            print(f'Epoch {epoch+1}: Loss {loss.item()}')\n","\n","train(model, train_loader, optimizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":843},"id":"GZo-IYzc43rD","executionInfo":{"status":"error","timestamp":1721776929799,"user_tz":-120,"elapsed":5010,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"09cd6731-31d6-49b3-e038-70490a84db10"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index 22 is out of bounds for dimension 3 with size 22","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-37ca6cf2173e>\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}: Loss {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-37ca6cf2173e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, epochs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-37ca6cf2173e>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mgrid_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mpadded_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mpadded_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten to match the output shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 22 is out of bounds for dimension 3 with size 22"]}]},{"cell_type":"code","source":["|import os\n","from PIL import Image\n","\n","base_dir = '/content/drive/My Drive/archive-5'\n","subsets = ['train', 'valid', 'test']\n","\n","# Check for any non-image or non-label files and missing pairs\n","for subset in subsets:\n","    images_dir = os.path.join(base_dir, subset, 'images')\n","    labels_dir = os.path.join(base_dir, subset, 'labels')\n","    images = set(os.listdir(images_dir))\n","    labels = set(os.listdir(labels_dir))\n","\n","    # Check for non-JPG images or non-txt label files\n","    non_jpgs = [img for img in images if not img.endswith('.jpg')]\n","    non_txts = [label for label in labels if not label.endswith('.txt')]\n","\n","    # Check for missing corresponding pairs\n","    image_without_label = [img for img in images if img.replace('.jpg', '.txt') not in labels]\n","    label_without_image = [label for label in labels if label.replace('.txt', '.jpg') not in images]\n","\n","    print(f\"{subset.upper()} SET:\")\n","    print(f\"Non-JPG files: {non_jpgs}\")\n","    print(f\"Non-txt label files: {non_txts}\")\n","    print(f\"Images without labels: {image_without_label}\")\n","    print(f\"Labels without images: {label_without_image}\")\n","    print('-'*50)\n","\n","# Optionally, check image dimensions\n","image_dimensions = set()\n","for image_file in os.listdir(os.path.join(base_dir, 'train', 'images')):\n","    with Image.open(os.path.join(base_dir, 'train', 'images', image_file)) as img:\n","        image_dimensions.add(img.size)\n","\n","print(\"Unique image dimensions:\", image_dimensions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"id":"IVp9bpOMPSsy","executionInfo":{"status":"error","timestamp":1721766077889,"user_tz":-120,"elapsed":275172,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"28cdbb45-e9fb-47b4-e5d6-551d5badfe8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN SET:\n","Non-JPG files: []\n","Non-txt label files: []\n","Images without labels: []\n","Labels without images: []\n","--------------------------------------------------\n","VALID SET:\n","Non-JPG files: []\n","Non-txt label files: []\n","Images without labels: []\n","Labels without images: []\n","--------------------------------------------------\n","TEST SET:\n","Non-JPG files: []\n","Non-txt label files: []\n","Images without labels: []\n","Labels without images: []\n","--------------------------------------------------\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-94db1bdbe152>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mimage_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mimage_dimensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Function to calculate the output size of each layer\n","def calculate_output_size(model, input_size):\n","    with torch.no_grad():  # No need to track gradients here\n","        input_tensor = torch.rand(1, *input_size)  # Generate a random input tensor\n","        output = model(input_tensor)\n","        return output.shape  # Return the shape of the output\n","\n","# Assuming your model is defined as follows\n","class TestModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n","            nn.MaxPool2d(2, 2),\n","            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            nn.Conv2d(192, 128, kernel_size=1),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.Conv2d(256, 256, kernel_size=1),\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n","            nn.MaxPool2d(2, 2),\n","            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n","            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n","            nn.Flatten()\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","# Create an instance of the model\n","model = TestModel()\n","\n","# Calculate and print the output size\n","output_size = calculate_output_size(model, (3, 416, 416))  # Assuming the input images are 416x416\n","print(\"Output size:\", output_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwiOB-pFHZ-D","executionInfo":{"status":"ok","timestamp":1721763737201,"user_tz":-120,"elapsed":1383,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"5631de49-8fec-41ac-c5dd-6297ad2b6d63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output size: torch.Size([1, 173056])\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","\n","def calculate_average_image_size(image_dir):\n","    widths, heights = [], []\n","    image_files = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith('.jpg')]\n","    for image_file in image_files:\n","        with Image.open(image_file) as img:\n","            width, height = img.size\n","            widths.append(width)\n","            heights.append(height)\n","    avg_width = sum(widths) / len(widths)\n","    avg_height = sum(heights) / len(heights)\n","    return avg_width, avg_height\n","\n","# Set the base directory for your dataset\n","base_dir = '/content/drive/My Drive/archive-5'\n","train_images_dir = os.path.join(base_dir, 'train', 'images')\n","\n","# Calculate and print the average image size\n","average_width, average_height = calculate_average_image_size(train_images_dir)\n","print(f'Average Image Width: {average_width}, Average Image Height: {average_height}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvUQwZXdVJPD","executionInfo":{"status":"ok","timestamp":1721684890297,"user_tz":-120,"elapsed":137172,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"5bcdaed7-df5b-4f55-e9bc-ed6be945af51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Image Width: 416.0, Average Image Height: 416.0\n"]}]},{"cell_type":"code","source":["# First, simulate the forwarding to get the output dimension\n","def get_conv_output_dim(input_dim, kernel_size, padding, stride):\n","    output_dim = (input_dim + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n","    return output_dim\n","\n","# Given your model architecture and initial input size of 416x416:\n","input_dim = 416\n","layer_configs = [\n","    (7, 2, 3),  # kernel_size, stride, padding\n","    (2, 2, 0),  # pooling\n","    (3, 1, 1),\n","    (2, 2, 0),  # pooling\n","    (1, 1, 0),\n","    (3, 1, 1),\n","    (1, 1, 0),\n","    (3, 1, 1),\n","    (2, 2, 0),  # pooling\n","    (1, 1, 0),\n","    (3, 1, 1),\n","    (1, 1, 0),\n","    (3, 1, 1),\n","    (2, 2, 0),  # pooling\n","    (3, 1, 1),\n","    (1, 1, 0),\n","    (3, 1, 1),\n","    (3, 1, 1),\n","    (2, 2, 0),  # pooling\n","    (3, 1, 1)\n","]\n","\n","for kernel_size, stride, padding in layer_configs:\n","    if kernel_size == 2:  # pooling layer\n","        input_dim = get_conv_output_dim(input_dim, kernel_size, padding, stride)\n","    else:  # convolution layer\n","        input_dim = get_conv_output_dim(input_dim, kernel_size, padding, stride)\n","\n","print(\"Final output dimension per dimension:\", input_dim)  # This should give you the size per dimension (height or width)\n"],"metadata":{"id":"suhJxFlqaHek","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721762829200,"user_tz":-120,"elapsed":328,"user":{"displayName":"Enxo Mucaj","userId":"02795068905167106125"}},"outputId":"c1d27042-f61e-4999-cade-b08bb4117777"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final output dimension per dimension: 6\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LKcbGuJnD9DA"},"execution_count":null,"outputs":[]}]}